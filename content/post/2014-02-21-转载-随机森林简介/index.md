---
title: 转载——随机森林简介
author: ''
date: '2014-02-23'
tags:
  - data science
slug: 转载-随机森林简介
---

**随机森林及其副产品**

随机森林(Random Forest)方法是Leo Breiman于2001年提出的一种集成学习（Ensemble Learning）方法，它是传统决策树方法的扩展，将多个决策树进行组合，来提高预测精度。随机森林利用分类回归树(CART)作为其基本组成单元，也可称之为基学习器或是子模型。而集成学习的思路是试图通过连续调用单个学习算法，获得不同的学习器，然后根据规则组合这些学习器来解决同一个问题， 可以显著的提高学习系统的泛化能力。组合多个学习器主要采用加权平均或投票的方法。常见的集成学习算法还包括了装袋算法（Bagging)和提升算法 （Boosting）。

1. 随机森林计算步骤

+ 从原始训练样本中随机有放回抽出N个样本；
+ 从解释变量中随机抽出M个变量；
+ 依据上述得到的子集实施CART方法（无需剪枝），从而形成一个单独的决策树；
+ 重复上面步骤X次，就构建了有X棵树的随机森林模型。
+ 在对新数据进行预测分类时，由X棵树分别预测，以投票方式综合最终结果。

2. 随机森林的特点

+ 相对其它算法，准确率很高；
+ 不会形成过度拟合；
+ 速度快，能够处理大数据；
+ 能处理很高维度的数据，不用做特征选择；
+ 由于抽样的原因，会有一些未被抽中的样本，这形成了所谓的袋外数据(OOB)，可以根据OOB来估计泛化误差，而不需要用交叉检验来估计。

3. 随机森林的副产品

除了能用于回归分类之外，它还可以提供一些其它很有价值的功能。R语言中的`randomForest`包中就包括了这些函数。

+ 副产品之一，判断变量的重要程度。

由于决策树是根据不同变量来分割数据，所以一棵树中能进行正确划分的变量就是最重要的变量。随机森林可以根据置换划分变量对分类误差的影响，来判断哪些变量是比较重要的。这个功能非常实用，特别在处理变量极多的数据集，可以用它来作为变量选择的过滤器，然后再使用其它分类方法。`randomForest`包中的`importance`函数能返回各变量的重要程度，`varImpplot`函数可以用图形方式加以展现。`partialPlot`函数则能呈现变量的偏效应。`rfcv`函数用来得到最优的的变量数目。

+ 副产品之二，度量样本间的相似程度。

决策树的理念是将数据归入不同的组中，那么同一组中的样本可以认为是比较相似的。根据这个思路可以建立起各样本间的相似矩阵。用1-相似矩阵则可以认为是一种“距离”，利用距离就可以进行异常值检验或聚类分析。`outlier`函数可以返回各样本的离群值，值越大表示越有可能是异常点。`cluster`包中的`pam`函数和`kmeans`函数相近，但它可以接受距离矩阵作为参数。

+ 副产品之三，缺失数据的插补。

处理缺失数据有一种初级方法，即对数值变量，用中位数来代替，对于分类变量，用频数最高的类来代替缺失值。`rfImpute`函数首先使用初级方法来插补，然后计算近似矩阵，再用近似度为权重再次加权计算缺失值。然后再次计算近似矩阵，这样反复迭代。`rfImpute`函数能实现这一功能。

+ 最后还值得提到的是处理不平衡数据。

可以在主函数`randomForest`中加入`classwt`参数设置，将数据较多的类设置为较大的权数，这样可以在一定程度上修正数据不平衡的影响，使占少数类别的预测准确率提高。当然这样做的代价是总误差水平上升了。

**参考资料：**
+ http://www.webchem.science.ru.nl/PRiNS/rF.pdf
+ http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm

**轉載自：**
+ http://xccds1977.blogspot.com/2012/07/blog-post_15.html

备注：转移自新浪博客，截至2021年11月，原阅读数180，评论0个。
